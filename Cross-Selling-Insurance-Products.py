# -*- coding: utf-8 -*-
"""Indradhar_Paka_1800315C203_Course_work_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16do80H762USzT3zk1ap0Ffb0GuVYzxqY
"""

"""%tensorflow_version 2.x
import tensorflow as tf
print("Tensorflow version " + tf.__version__)

try:
  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
except ValueError:
  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')

tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"""
import torch
torch.cuda.is_available()
device=torch.device("cuda:0")
device
if torch.cuda.is_available():
  device=torch.device("cuda:0")
  print("running on gpu")
else:
  device=torch.device("cpu")
  print("running on cpu")

torch.cuda.device_count()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

data = pd.read_csv("/content/Insurance cross sell.csv")

type(data)

data

output_var = 'Response'

import numpy as np
X = data[data.columns[~data.columns.isin([output_var])]]
y = data[[output_var]]

from sklearn.model_selection import train_test_split
np.random.seed(37) # Set seed
x_train, x_test = train_test_split(X, test_size = 0.25)

## dont using iloc as we have projected from data
y_train = y.loc[x_train.index.values] 
y_test = y.loc[x_test.index.values]
x_train = X.loc[x_train.index.values, :]
x_test = X.loc[x_test.index.values, :]

concat_train_data = pd.concat([x_train,y_train],axis=1)

concat_test_data = pd.concat([x_test,y_test],axis=1)

x_train

y_train

x_test

y_test

"""# 1. **Undertake Exploratory Data Analysis to identify patterns in the data to discover insights that could help you build better models**"""

y_train.value_counts().plot.pie(autopct = '%1.1f%%',colors=['Orange','Blue'], figsize = (7,7))

"""

> Data is Imbalanced. Only 12.3% of customers are likely to buy insuarance




"""

data.isna().sum()/data.shape[0]*100

"""> >There are no missing values

Analysing Each Variable's Relationship with Target/output Variable

1. Gender
"""

import seaborn as sns
sns.countplot(x_train['Gender'], hue=y_train['Response'] ,palette=['Orange','Purple'])

"""2. Age"""

f,ax = plt.subplots(nrows=2,ncols=1,figsize=(30,10))
axx = ax.flatten()
#plt.figure(figsize=(30,10))
sns.distplot(x_train['Age'],ax=axx[0], color='Blue')
sns.boxplot(x_train['Age'],ax=axx[1],color='Orange')

"""Analysing Response for different Age-Groups"""

age_grp_20_to_30 = concat_train_data[concat_train_data['Age'] <31]
age_grp_31_to_40 = concat_train_data[ concat_train_data['Age'].between(31,40)]
age_grp_41_to_50 = concat_train_data[concat_train_data['Age'].between(41,50)]
age_grp_50_to_60 = concat_train_data[ concat_train_data['Age'].between(51,60)]
age_grp_old = concat_train_data[ concat_train_data['Age'] >60]

age_grp = [age_grp_20_to_30,age_grp_31_to_40,age_grp_41_to_50,age_grp_50_to_60,age_grp_old]
age_grp_name = ['age_grp_20_to_30','age_grp_31_to_40','age_grp_41_to_50','age_grp_50_to_60','age_grp_old']
age_grp_dict = dict(zip(age_grp_name, age_grp))

f,ax = plt.subplots(nrows=2, ncols=3, figsize = (20,10))
axx = ax.flatten()
for pos,tup in enumerate(age_grp_dict.items()):
    axx[pos].set_title(tup[0])
    data = tup[1]
    concat_train_data['Response'].value_counts().plot.pie(autopct='%1.1f%%', ax = axx[pos],colors=['Red','Blue'])

"""Analysing Response with both 'Age-Groups' and 'Gender'"""

f,ax = plt.subplots(nrows=2, ncols=3, figsize = (20,10))
axx = ax.flatten()
plt.title('Response Percentage of Different Age Groups with Genders',fontsize=40,x=-0.5,y=2.5)
for pos,tup in enumerate(age_grp_dict.items()):
    axx[pos].set_title(tup[0])
    temp = tup[1]
    temp.groupby('Gender')['Response'].value_counts().plot.pie(autopct='%1.1f%%', ax = axx[pos],colors=['Orange','Purple'])

sns.catplot(x = 'Gender', y="Age",hue = 'Response', data=concat_train_data)

"""*   Customers of age between 30 to 60 are more likely to buy insurance.

*   Customes of age between 20 to 30 are less likely to buy insurance.

*   In almost every age group, 'Male's are more likely to buy insurance.

*   Females under age 30 are very less likely to buy insurance

3. Driving_License
"""

x_train['Driving_License'].value_counts().plot.pie(autopct='%1.1f%%',colors = ['Blue','Red'])

f,ax = plt.subplots(nrows=1,ncols=2,figsize = (20,5))
axx = ax.flatten()
#plt.title('Driving_License wise Response',fontsize=40,x=-0.5,y=2)
axx[0].set_title('Driving_Licence = 1')
axx[1].set_title('Driving_Licence = 0')
concat_train_data[ concat_train_data['Driving_License'] == 1]['Response'].value_counts().plot.pie(autopct='%1.1f%%',colors = ['Blue','Red'],ax=axx[0])
concat_train_data[ concat_train_data['Driving_License'] == 0]['Response'].value_counts().plot.pie(autopct='%1.1f%%',colors = ['Blue','Red'],ax=axx[1])

"""*   Very few customers don't have Driving License.
*   Customers with Driving License have higher chance of buying Insurance

4. REGION CODE
"""

plt.figure(figsize = (40,10))
plt.title('Region Wise Response Count',fontsize=50)
sns.countplot(x_train['Region_Code'], hue = y_train['Response'],palette=['Red','Blue'])

"""Visualizing Percentage of Response : 1 in all Region_Codes"""

u_region = x_train['Region_Code'].unique()
region_perc = {}
for i in u_region:
    total_region = x_train[ x_train['Region_Code'] == i].shape[0]
    buy_region = x_train[ (x_train['Region_Code'] == i) & y_train['Response'] == 1].shape[0]
    region_perc[i] = (buy_region/total_region)*100

region_perc = sorted(region_perc.items(), key=lambda x: x[1], reverse=True)
region_perc = list(zip(*region_perc))

region = np.array(region_perc[0])
region_perc = np.array(region_perc[1])
region = pd.DataFrame(region)
region_perc = pd.DataFrame(region_perc)

region_res_perc = pd.concat((region,region_perc), axis=1)
region_res_perc.columns = ['Region_Code', 'Buy_Percentage']

plt.figure(figsize=(40,10))
plt.title('Region Wise Buying Percentage',fontsize=50)
ax = sns.barplot(x = region_res_perc['Region_Code'], y = region_res_perc['Buy_Percentage'])

"""*   We have most of the customers from Region_Code : 28.
*   Region_Codes: [4,19,23,24,,28,38,51] have higher percentage of buying insurance.
*   Region_Codes: 25 and 44 have lower percentage of buying insurance.

5. Previously_Insuaranced
"""

plt.figure(figsize=(15,5))
sns.countplot(x_train['Previously_Insured'],hue=y_train['Response'],palette=['Brown','Purple'])

"""*   Customers who Previously_Insured are very likely to buy Insurnce now.
*   Customers who didn't Previously_Insured have good chance of buying Insurnce.

6. Vehicle_Age
"""

plt.figure(figsize=(7,7))
x_train['Vehicle_Age'].value_counts().plot.pie(autopct='%1.1f%%', colors = ['r', 'b', 'g'])

plt.figure(figsize = (30,10))
sns.countplot(x_train['Vehicle_Age'], hue = y_train['Response'])

ls = x_train['Vehicle_Age'].unique()

f,ax = plt.subplots(nrows=1, ncols=3,figsize = (30,10))
axx = ax.flatten()
for pos,val in enumerate(ls):
    axx[pos].set_title(str(val))
    concat_train_data[x_train['Vehicle_Age'] == val]['Response'].value_counts().plot.pie(autopct = '%1.1f%%',ax = axx[pos], colors=['Purple', 'Orange'])

"""*   We have half of our customers with Vehicle_Age 1-2 years.
*   We have very few customers (4.2%) with Vehicle_Age `>2 years.
*   Customers with Vehicle_Age >2years have better chance (29.4%) of buying Insurance.
*   Customers with with Vehicle_Age <1 years have very less chance of buying Insurance.

7. Vehicle_Damage
"""

sns.countplot(x_train['Vehicle_Damage'], hue = y_train['Response'])

"""*   We have almost same number of customes with damaged and non_damaged vehicle.
*  Customers with Vehicle_Damage are likely to buy insurance.
*  Customers with non damaged vehicle have least chance (less than 1%) of buying insurance.

8. Annual_Premium
"""

f,ax = plt.subplots(nrows=2,ncols=1,figsize=(30,20))
axx = ax.flatten()
#plt.figure(figsize=(30,10))
sns.distplot(x_train['Annual_Premium'],ax=axx[0], color='Blue')
sns.boxplot(x_train['Annual_Premium'],ax=axx[1],color='Orange')

plt.figure(figsize=(40,10))
sns.distplot(concat_train_data[ x_train['Annual_Premium'] < 100000]['Annual_Premium'])#.plot.hist(bins = 500, frequency=(0,10000))

start = 0
step = 10000
ls = []
for _ in range(10):
    ls.append((start,step))
    start = step
    step+=10000

for tup in ls:
    count = x_train[ x_train['Annual_Premium'].between(tup[0],tup[1])].shape[0]
    percentage = x_train[ (x_train['Annual_Premium'].between(tup[0], tup[1])) & (y_train['Response'] == 1)].shape[0]/x_train[ x_train['Annual_Premium'].between(tup[0], tup[1])].shape[0]*100
    print('Number of Customers with Annual_Premium Between {} : {} and Insurance Buy Percentage:{}'.format(tup,count,percentage))

"""*  'Annual Premium' data is highlt left skewed.
*  Most of the customers have "Annual_Premium' in range (0, 10000) and (20000 to 50000)
*  In every 'Annual Premium' range, the insurance buy percentage is almost same.

9. Plolicy_Sales_Channel
"""

plt.figure(figsize=(40,10))
x_train['Policy_Sales_Channel'].value_counts().plot.bar()

"""*  Policy_Sales_Channel no. 152 have higest number of customers.
*  Policy_Sales_Channel no. [152,26,124,160,156,122,157,154,151,163] have most of the customers.

10. Vintage
"""

f,ax = plt.subplots(nrows=2,ncols=1,figsize=(30,20))
axx = ax.flatten()
sns.distplot(x_train['Vintage'],ax=axx[0], color='Blue')
sns.boxplot(x_train['Vintage'],ax=axx[1],color='Orange')

"""Every 'Vintage' value have almost same number of customers.

Understandings:
* Customers of age between 30 to 60 are more likely to buy insurance.
* Customes of age between 20 to 30 are less likely to buy insurance.
* In almost every age group, 'Male's are more likely to buy insurance.
* Females under age 30 are very less likely ho buy insurance.
* Very few customers don't have Driving License.
* Customers with Driving License have higher chance of buying Insurance.
* We have most of the customers from Region_Code : 28.
* Region_Codes: [4,19,23,24,,28,38,51] have higher percentage of buying insurance.
* Region_Codes: 25 and 44 have lower percentage of buying insurance.
* Customers who Previously_Insured are very likely to buy Insurnce now.
* Customers who didn't Previously_Insured have good chance of buying Insurnce.
* We have half of our customers with Vehicle_Age 1-2 years.
* We have very few customers (4.2%) with Vehicle_Age >2 years.
* Customers with Vehicle_Age >2years have better chance (29.4%) of buying Insurance.
* Customers with with Vehicle_Age <1 years have very less chance of buying Insurance.
* We have almost same number of customes with damaged and non_damaged vehicle.
* Customers with Vehicle_Damage are likely to buy insurance.
* Customers with non damaged vehicle have least chance (less than 1%) of buying insurance.
* 'Annual Premium' data is highlt left skewed.
* Most of the customers have "Annual_Premium' in range (0, 10000) and (20000 to 50000)
* In every 'Annual Premium' range, the insurance buy percentage is almost same.
* Policy_Sales_Channel no. 152 have higest number of customers.
* Policy_Sales_Channel no. [152,26,124,160,156,122,157,154,151,163] have most of the customers.
* Every 'Vintage' value have almost same number of customers.

Outlier Analysis:

1. Age
"""

sns.boxplot('Age', data=data, orient='v', color='Red')

"""2. Annual Premium"""

sns.boxplot('Annual_Premium', data=data,orient='v', color='red')

"""Corretation Analysis:"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
corr_check = data.copy()

col_ls = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']

for col in col_ls:
    corr_check[col] = le.fit_transform(corr_check[col])

plt.figure(figsize=(20,10))
sns.heatmap(corr_check.corr(), annot=True, square=True,annot_kws={'size': 10})

"""* 'Previously_Insured' and 'Vehicle_Damage' are highly positively corelated.
* 'Age' and 'Policy_Sales_Channel' are negatively corelated.
* 'Age' and 'Vehicle_Age' are negatively corelated.

---

# 2. Build models using the standard classification algorithms that you have studied during the course i.e. logistic regression, k-nearest neighbour, naive Bayes, decision trees, support vector machines, random forest and gradient boosted decision trees
"""

concat_train_data['Vehicle_Age']=concat_train_data['Vehicle_Age'].replace({'< 1 Year':0,'1-2 Year':1,'> 2 Years':2})
concat_train_data['Gender']=concat_train_data['Gender'].replace({'Male':1,'Female':0})
concat_train_data['Vehicle_Damage']=concat_train_data['Vehicle_Damage'].replace({'Yes':1,'No':0})

concat_test_data['Vehicle_Age']=concat_test_data['Vehicle_Age'].replace({'< 1 Year':0,'1-2 Year':1,'> 2 Years':2})
concat_test_data['Gender']=concat_test_data['Gender'].replace({'Male':1,'Female':0})
concat_test_data['Vehicle_Damage']=concat_test_data['Vehicle_Damage'].replace({'Yes':1,'No':0})

features=['Gender', 'Age', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']

x=concat_train_data[features]

np.random.seed(37) # Set seed
X_train, X_test = train_test_split(x, test_size = 0.25)

## dont using iloc as we have projected from data
Y_train = y.loc[X_train.index.values] 
Y_test = y.loc[X_test.index.values]
X_train = x.loc[X_train.index.values, :]
X_test = x.loc[X_test.index.values, :]

X_train

from sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report

"""1. Logistic Regression Model"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train,Y_train)
Y_pred = lr.predict(X_test)

proba = lr.predict_proba(X_test)[:, 1]

print(classification_report(Y_test, Y_pred))
print('Logistic Regression Base Accuracy : {}'.format(accuracy_score(Y_test,Y_pred)))
print('Logistic Regression Base ROC_AUC_SCORE: {}'.format(roc_auc_score(Y_test,proba)))

"""2. K-nearest neighbour Model"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train,Y_train)
Y_pred = knn.predict(X_test)
proba = knn.predict_proba(X_test)[:, 1]

print(classification_report(Y_test, Y_pred))
print('KNN Base Accuracy : {}'.format(accuracy_score(Y_test,Y_pred)))
print('KNN Base ROC_AUC_SCORE: {}'.format(roc_auc_score(Y_test,proba)))

"""3. Naive bayes Model"""

from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(X_train,Y_train)
Y_pred = nb.predict(X_test)
proba = nb.predict_proba(X_test)[:, 1]

print(classification_report(Y_test, Y_pred))
print('Naive Bayes Base Accuracy : {}'.format(accuracy_score(Y_test,Y_pred)))
print('Naive Bayes Base ROC_AUC_SCORE: {}'.format(roc_auc_score(Y_test,proba)))

"""4. Decision Trees Model"""

from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree.fit(X_train,Y_train)
Y_pred = dtree.predict(X_test)
proba = dtree.predict_proba(X_test)[:, 1]

print(classification_report(Y_test, Y_pred))
print('Decision Trees Base Accuracy : {}'.format(accuracy_score(Y_test,Y_pred)))
print('Decision Trees Base ROC_AUC_SCORE: {}'.format(roc_auc_score(Y_test,proba)))

"""5. Support Vector Machines"""

from sklearn.svm import SVC
svm = SVC()
svm.fit(X_train, Y_train)
Y_pred = svm.predict(X_test)
#proba = svm.predict_proba(X_test)[:, 1]

print(classification_report(Y_test, Y_pred))
#print('SVM Base Accuracy : {}'.format(accuracy_score(Y_test,Y_pred)))
#print('SVM Base ROC_AUC_SCORE: {}'.format(roc_auc_score(Y_test,proba)))

"""6. Random Forest Model"""

from sklearn.ensemble import RandomForestClassifier
rfm = RandomForestClassifier()
rfm.fit(X_train,Y_train)
Y_pred = rfm.predict(X_test)
proba = rfm.predict_proba(X_test)[:, 1]

print(classification_report(Y_test, Y_pred))
print('Random Forest Base Accuracy : {}'.format(accuracy_score(Y_test,Y_pred)))
print('Random Forest Base ROC_AUC_SCORE: {}'.format(roc_auc_score(Y_test,proba)))

"""7. Gradient boosted Decision Trees"""

from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(n_estimators=20, learning_rate = 0.1, max_features=2, max_depth = 2, random_state = 0)
gb.fit(X_train, Y_train)
Y_pred = gb.predict(X_test)
proba = gb.predict_proba(X_test)[:, 1]

print(classification_report(Y_test, Y_pred))
print('Gradient boosted Base Accuracy : {}'.format(accuracy_score(Y_test,Y_pred)))
print('Gradient boosted Base ROC_AUC_SCORE: {}'.format(roc_auc_score(Y_test,proba)))

"""---

# 3. Noting the skew in the distribution, study methods for addressing the skew using over or under-sampling and SMOTE and apply them to the problem
"""

concat_train_data

def upsample(df, u_feature, n_upsampling):
    ones = df.copy()
    for n in range(n_upsampling):
        if u_feature == 'Annual_Premium':
            df[u_feature] = ones[u_feature].apply(lambda x: x + random.randint(-1,1)* x *0.05) # change Annual_premiun in the range of 5%
        else:
            df[u_feature] = ones[u_feature].apply(lambda x: x + random.randint(-5,5)) # change Age in the range of 5 years
                
        if n == 0:
            df_new = df.copy()
        else:
            df_new = pd.concat([df_new, df])
    return df_new

df_train_mod = concat_train_data.copy()
df_train_mod['old_damaged'] = df_train_mod.apply(lambda x: pow(2,x.Vehicle_Age)+pow(2,x.Vehicle_Damage), axis =1)

import random
SEED = 1970
random.seed(SEED)
# we shall preserve validation set without augmentation/over-sampling
df_temp, X_valid, _, y_valid = train_test_split(df_train_mod, df_train_mod['Response'], train_size=0.8, random_state = SEED)
X_valid = X_valid.drop(columns = ['Response'])

# upsampling Positive Response class only
df_train_up_a = upsample(df_temp.loc[df_temp['Response'] == 1], 'Age', 1)
df_train_up_v = upsample(df_temp.loc[df_temp['Response'] == 1], 'Vintage', 1)

df_train_mod.head()

df_ext = pd.concat([df_train_mod,df_train_up_a])
df_ext = pd.concat([df_ext,df_train_up_v])
train_X = df_ext.drop(columns = ['Response'])
train_Y = df_ext.Response
print('Train set target class count with over-sampling:')
print(train_Y.value_counts())
print('Validation set target class count: ')
print(y_valid.value_counts())
train_X.head()

_

"""---

# 4. Use methods discussed in class for hyperparameter tuning of the models

Now we are going to check how the models work with the same parameters as baseline will predict using upsampled data with new features.

1. Logistic Regression
"""

lr = LogisticRegression(max_iter=1000)
lr.fit(train_X,train_Y)
Y_pred = lr.predict(X_valid)
proba = lr.predict_proba(X_valid)[:, 1]

print(classification_report(y_valid, Y_pred))
print('Logistic Regression After tuning Accuracy : {}'.format(accuracy_score(y_valid,Y_pred)))
print('Logistic Regression After tuning ROC_AUC_SCORE: {}'.format(roc_auc_score(y_valid,proba)))

"""2.  k-nearest neighbour"""

knn = KNeighborsClassifier()
knn.fit(train_X,train_Y)
Y_pred = knn.predict(X_valid)
proba = knn.predict_proba(X_valid)[:, 1]

print(classification_report(y_valid, Y_pred))
print('KNN After tuning Accuracy : {}'.format(accuracy_score(y_valid,Y_pred)))
print('KNN After tuning  ROC_AUC_SCORE: {}'.format(roc_auc_score(y_valid,proba)))

"""3. Naive Bayes Model"""

nb = GaussianNB()
nb.fit(train_X,train_Y)
Y_pred = nb.predict(X_valid)
proba = nb.predict_proba(X_valid)[:, 1]

print(classification_report(y_valid, Y_pred))
print('Naive Bayes After tuning  Accuracy : {}'.format(accuracy_score(y_valid,Y_pred)))
print('Naive Bayes After tuning  ROC_AUC_SCORE: {}'.format(roc_auc_score(y_valid,proba)))

"""4. Decision Tree Model"""

dtree = DecisionTreeClassifier()
dtree.fit(train_X,train_Y)
Y_pred = dtree.predict(X_valid)
proba = dtree.predict_proba(X_valid)[:, 1]

print(classification_report(y_valid, Y_pred))
print('Decision Trees After Tuning Accuracy : {}'.format(accuracy_score(y_valid,Y_pred)))
print('Decision Trees After Tuning ROC_AUC_SCORE: {}'.format(roc_auc_score(y_valid,proba)))

"""5. Support Vector Machines"""

svm = SVC()
svm.fit(train_X, train_Y)
Y_pred = svm.predict(X_valid)
print(classification_report(y_valid, Y_pred))

"""6. Random Forest Model"""

rfm = RandomForestClassifier()
rfm.fit(train_X,train_Y)
Y_pred = rfm.predict(X_valid)
proba = rfm.predict_proba(X_valid)[:, 1]

print(classification_report(y_valid, Y_pred))
print('Random Forest After Tuning Accuracy : {}'.format(accuracy_score(y_valid,Y_pred)))
print('Random Forest After Tuning ROC_AUC_SCORE: {}'.format(roc_auc_score(y_valid,proba)))

"""7. Gradient Boosted Decision Trees"""

gb = GradientBoostingClassifier(n_estimators=20, learning_rate = 0.1, max_features=2, max_depth = 2, random_state = 0)
gb.fit(train_X, train_Y)
Y_pred = gb.predict(X_valid)
proba = gb.predict_proba(X_valid)[:, 1]

print(classification_report(y_valid, Y_pred))
print('Gradient boosted After Tuning Accuracy : {}'.format(accuracy_score(y_valid,Y_pred)))
print('Gradient boosted Base ROC_AUC_SCORE: {}'.format(roc_auc_score(y_valid,proba)))

"""**We will now tune the hyperparameters**

1. Logistic Regression
"""

solvers = ['newton-cg', 'lbfgs', 'liblinear']
penalty = ['l2']
c_values = [100, 10, 1.0, 0.1, 0.01]

grid = dict(solver=solvers,penalty=penalty,C=c_values)

from sklearn.model_selection import GridSearchCV
lr_grid = GridSearchCV(estimator=lr, param_grid = grid, cv = 3, verbose=True, n_jobs=-1,scoring='accuracy',error_score=0)
best_lr = lr_grid.fit(X_train,Y_train)

lr_grid.best_params_

#print("Best Parameters:",rf_grid.best_params_)
print("Best Estimator:",lr_grid.best_estimator_)
print("Accuracy Score:",lr_grid.score(X_test,Y_test))

"""2. K- Nearest Neighbour"""

leaf_size = list(range(1,30))
n_neighbors = list(range(1,10))
p=[1,2]

parameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)
knn_grid = GridSearchCV(knn, param_grid=parameters, cv=2,verbose=True,n_jobs=-1)
knn_grid.fit(X_train,Y_train)
knn_grid.best_params_

"""3. Naive Bayes Model"""

parameters = {
    'var_smoothing': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]
}
nb_grid = GridSearchCV(estimator=nb,param_grid=parameters, cv = 2, verbose=True, n_jobs=-1,scoring='accuracy')
nb_grid.fit(X_train,Y_train)
nb_grid.best_params_

"""4. Decision Tree Model"""

max_features = ['auto', 'sqrt','log2']
max_depth = [int(x) for x in np.linspace(10, 1000,10)]

min_samples_split = [2, 5, 10,14]
min_samples_leaf = [1, 2, 4,6,8]

params = {
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf
              }
dt_grid = GridSearchCV(estimator=dtree,param_grid = params, cv = 2, verbose=True, n_jobs=-1,scoring='accuracy',error_score=0)
dt_grid.fit(X_train,Y_train)
dt_grid.best_params_

"""5. Support Vector Machines"""

cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
space = dict()
space['max_iter'] = [1,10,50,100]
space['kernel']= ['linear','poly','rbf','sigmoid']
space['gamma'] = [1, 0.1, 0.01, 0.001, 0.0001]
space['C']: [0.1, 1, 10, 100, 1000]

search = GridSearchCV(sv, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)

search.fit(X_train, Y_train)
search.best_params_
#print("Best Parameters:",rf_grid.best_params_)
print("Best Estimator:",search.best_estimator_)
print("Accuracy Score:",search.score(X_test,Y_test))

"""6. Random Forest Model"""

max_features = ['auto', 'sqrt','log2']
max_depth = [int(x) for x in np.linspace(10, 1000,10)]

min_samples_split = [2, 5, 10,14]
min_samples_leaf = [1, 2, 4,6,8]

params = {
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf
              }
rf_grid = GridSearchCV(estimator=rfm,param_grid = params, cv = 2, verbose=True, n_jobs=-1,scoring='accuracy')
rf_grid.fit(X_train,Y_train)
print("Best Parameters:",rf_grid.best_params_)
print("Best Estimator:",rf_grid.best_estimator_)
print("Accuracy Score:",rf_grid.score(X_test,Y_test))

"""7. Gradient Boosted Decision Trees"""

max_depth = [int(x) for x in np.linspace(10, 100,5)]
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4,6]
max_features = ['auto']
criterion=['friedman_mse']

parameters = {
               'criterion':criterion,
               
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf
              }
gb_grid = GridSearchCV(gb, param_grid=parameters, cv=2,verbose=True,n_jobs=-1)
gb_grid.fit(X_train,Y_train)

"""# 5. Using area under ROC curve to select the best performing model"""

def plot_ROC(fpr, tpr, m_name):
    roc_auc = sklearn.metrics.auc(fpr, tpr)
    plt.figure(figsize=(6, 6))
    lw = 2
    plt.plot(fpr, tpr, color='darkorange',
             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc, alpha=0.5)
    
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', alpha=0.5)
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xticks(fontsize=16)
    plt.yticks(fontsize=16)
    plt.grid(True)
    plt.xlabel('False Positive Rate', fontsize=16)
    plt.ylabel('True Positive Rate', fontsize=16)
    plt.title('Receiver operating characteristic for %s'%m_name, fontsize=20)
    plt.legend(loc="lower right", fontsize=16)
    plt.show()
    #need to be executed

"""1. Logistic Regression"""

import sklearn
from sklearn.metrics import auc, roc_curve
lr_preds_u = lr.predict_proba(X_valid)
lr_score_u = roc_auc_score(y_valid, lr_preds_u[:,1])
lr_class_u = lr.predict(X_valid)

(fpr, tpr, thresholds) = roc_curve(y_valid, lr_preds_u[:,1])
plot_ROC(fpr, tpr,'Logistic')

!pip install scikit-plot

import scikitplot as skplt
print('ROC AUC score for Logistic model with over-sampling: %.4f'%lr_score_u)
print('F1 score: %0.4f'%f1_score(y_valid, lr_class_u))
skplt.metrics.plot_confusion_matrix(y_valid, lr_class_u,
        figsize=(8,8))

"""Now we got much better True Positives, and quite acceptable AUC and f1 scores

2. k- Nearest Neighbour
"""

knn_preds_u = knn.predict_proba(X_valid)
knn_score_u = roc_auc_score(y_valid, knn_preds_u[:,1])
knn_class_u = knn.predict(X_valid)

(fpr, tpr, thresholds) = roc_curve(y_valid, knn_preds_u[:,1])
plot_ROC(fpr, tpr,'KNN')

print('ROC AUC score for KNN with over-sampling: %.4f'%knn_score_u)
print('F1 score: %0.4f'%f1_score(y_valid, knn_class_u))
skplt.metrics.plot_confusion_matrix(y_valid, knn_class_u,
        figsize=(8,8))

"""3. Naive Baye's"""

nb_preds_u = nb.predict_proba(X_valid)
nb_score_u = roc_auc_score(y_valid, nb_preds_u[:,1])
nb_class_u = nb.predict(X_valid)

(fpr, tpr, thresholds) = roc_curve(y_valid, nb_preds_u[:,1])
plot_ROC(fpr, tpr,'Naive Bayes')

print('ROC AUC score for Naive Bayes with over-sampling: %.4f'%nb_score_u)
print('F1 score: %0.4f'%f1_score(y_valid, nb_class_u))
skplt.metrics.plot_confusion_matrix(y_valid, nb_class_u,
        figsize=(8,8))

"""4. Decision Tree Model"""

dtree_preds_u = dtree.predict_proba(X_valid)
dtree_score_u = roc_auc_score(y_valid, dtree_preds_u[:,1])
dtree_class_u = dtree.predict(X_valid)

(fpr, tpr, thresholds) = roc_curve(y_valid, dtree_preds_u[:,1])
plot_ROC(fpr, tpr,'Decision tree')

print('ROC AUC score for Decision Tree with over-sampling: %.4f'%dtree_score_u)
print('F1 score: %0.4f'%f1_score(y_valid, dtree_class_u))
skplt.metrics.plot_confusion_matrix(y_valid, dtree_class_u,
        figsize=(8,8))

"""5. Support Vector Machines

svm_preds_u = svm.predict_proba(X_valid)
svm_score_u = roc_auc_score(y_valid, svm_preds_u[:,1])
svm_class_u = svm.predict(X_valid)
(fpr, tpr, thresholds) = roc_curve(y_valid, svm_preds_u[:,1])
plot_ROC(fpr, tpr,'SVM')

6. Random Forest
"""

rfm_preds_u = rfm.predict_proba(X_valid)
rfm_score_u = roc_auc_score(y_valid, rfm_preds_u[:,1])
rfm_class_u = rfm.predict(X_valid)

print('ROC AUC score for Decision Tree with over-sampling: %.4f'%rfm_score_u)
print('F1 score: %0.4f'%f1_score(y_valid, rfm_class_u))
skplt.metrics.plot_confusion_matrix(y_valid, rfm_class_u,
        figsize=(8,8))

(fpr, tpr, thresholds) = roc_curve(y_valid, rfm_preds_u[:,1])
plot_ROC(fpr, tpr,'Random Forest')

"""7.  gradient boosted decision trees """

gb_preds_u = gb.predict_proba(X_test)
gb_score_u = roc_auc_score(Y_test, gb_preds_u[:,1])
gb_class_u = gb.predict(X_test)

(fpr, tpr, thresholds) = roc_curve(Y_test, gb_preds_u[:,1])
plot_ROC(fpr, tpr,'Gradient Boosted')